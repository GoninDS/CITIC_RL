{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3DmoKPZp-rb"
      },
      "source": [
        "\n",
        "# Ejemplo de Blackjack con Q-Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este ejemplo originalmente pertenece a Till Zeemann y está publicado en la [documentación de Gymnasium](https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/), pero fue modificado bajo *fair use* debido a que pertenece a la licensia de MIT para la utilización en el taller."
      ],
      "metadata": {
        "id": "SghBi4g3rbTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![CITIC](https://inil.ucr.ac.cr/wp-content/uploads/2023/06/CITIC.jpg)\n",
        "\n",
        "Modificado para el taller por:\n",
        "- Luis David Solano Santamaría"
      ],
      "metadata": {
        "id": "xZh_glkFrw5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contexto"
      ],
      "metadata": {
        "id": "Gl0Nloptr5jK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "¡Una vez más vamos a ver las maravillas de Q-Learning, en este caso con este ejemplo de un ambiente de gymnasium que modela Blackjack! Para motivos de este ejemplo, asumo que ya observaron el anterior de Frozen Lake, para que tomen eso en cuenta si no lo han hecho.\n",
        "</p>"
      ],
      "metadata": {
        "id": "zGtGQFeqr_ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problema a resolver\n",
        "\n"
      ],
      "metadata": {
        "id": "poI3ei7_sV7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Agent](https://gymnasium.farama.org/_images/blackjack_AE_loop_dark.png)\n",
        "\n",
        "<p align=\"justify\">\n",
        "Observemos que queremos construir un agente que va a interactuar con el ambiente para jugar Blackjack y ganarle a un dealer. Como siempre, tenemos todas las partes clasicas: observaciones, recompensas y acciones.\n",
        "</p>\n",
        "\n",
        "![Environment](https://gymnasium.farama.org/_images/blackjack.gif)\n",
        "\n",
        "<p align=\"justify\">\n",
        "Para entender entonces este problema debemos comprender lo siguiente:\n",
        "</p>\n",
        "\n",
        "- ¿Cómo jugar Blackjack?\n",
        "- Espacio de acción\n",
        "- Espacio de observación\n",
        "- Estado inicial\n",
        "- Recompensa\n",
        "- Condición de parada"
      ],
      "metadata": {
        "id": "8byo9wZlsb9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Cómo jugar Blackjack?"
      ],
      "metadata": {
        "id": "cZ4DTEU5tbZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">El juego comienza con el dealer teniendo una carta boca arriba y una boca abajo, mientras que el jugador tiene dos cartas boca arriba. Todas las cartas se sacan de un mazo infinito (es decir, con reemplazo).</p>\n",
        "\n",
        "<p align=\"justify\">Los valores de las cartas son los siguientes:</p>\n",
        "\n",
        "<p align=\"justify\">- Las cartas de figura (J, Q, K) tienen un valor de 10 puntos.</p>\n",
        "<p align=\"justify\">- Los Ases pueden contar como 11 (llamado ‘As utilizable’) o como 1.</p>\n",
        "<p align=\"justify\">- Las cartas numéricas (2-10) tienen un valor igual a su número.</p>\n",
        "\n",
        "<p align=\"justify\">El jugador tiene la suma de las cartas que posee. El jugador puede solicitar cartas adicionales (pedir) hasta que decida detenerse (plantarse) o superar 21 (pasarse, lo que resulta en una pérdida inmediata).</p>\n",
        "\n",
        "<p align=\"justify\">Después de que el jugador se plante, el crupier revela su carta oculta y saca cartas hasta que su suma sea 17 o mayor. Si el crupier se pasa, el jugador gana.</p>\n",
        "\n",
        "<p align=\"justify\">Si ni el jugador ni el crupier se pasan, el resultado (ganar, perder, empate) se decide según quién esté más cerca de 21.</p>\n",
        "\n",
        "<p align=\"justify\">Este entorno corresponde a la versión del problema de blackjack descrita en el Ejemplo 5.1 del libro Reinforcement Learning: An Introduction de Sutton y Barto.</p>\n"
      ],
      "metadata": {
        "id": "JgkmDkgQteB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Espacio de acción"
      ],
      "metadata": {
        "id": "4uAAx7a2tzfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El espacio de acción es simple en naturaleza, podemos parar de tomar cartas o tomar una carta más. Esto lo pasamos a números de la siguiente manera:\n",
        "\n",
        "0. Parar de tomar cartas (Stick)\n",
        "\n",
        "1. Tomar una carta más (Hit)"
      ],
      "metadata": {
        "id": "SvgMrxgmt0l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Espacio de observación"
      ],
      "metadata": {
        "id": "EbOy7nlZuCx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "La observación consiste en un tupla de tres valores que contiene:\n",
        "</p>\n",
        "\n",
        "1. La suma actual del jugador\n",
        "\n",
        "2. El valor de la carta visible del dealer (1-10 donde 1 es un as)\n",
        "\n",
        "3. Si el jugador tiene un as utilizable (0 o 1).\n",
        "\n"
      ],
      "metadata": {
        "id": "XVcx-w1IuMOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estado inicial"
      ],
      "metadata": {
        "id": "VJjBmYnBucRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Observación              | Valores         |\n",
        "|--------------------------|-----------------|\n",
        "| Suma actual del jugador  | 4, 5, …, 21     |\n",
        "| Valor de la carta visible del dealer | 1, 2, …, 10  |\n",
        "| As utilizable            | 0, 1            |\n"
      ],
      "metadata": {
        "id": "q6UwcnVOudd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recompensas\n",
        "\n"
      ],
      "metadata": {
        "id": "5fS48KkhuqQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ganar el juego: +1\n",
        "\n",
        "- Perder juego: -1\n",
        "\n",
        "- Empatar juego: 0"
      ],
      "metadata": {
        "id": "HaWm5S_Ru5rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Condición de parada\n"
      ],
      "metadata": {
        "id": "pI1Uw42lvDE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un episodio puede terminar si ocurren los siguientes dos eventos:\n",
        "\n",
        "1. El jugador excede 21 y pierde.\n",
        "2. El jugador abandona (sticks)."
      ],
      "metadata": {
        "id": "oaDmSEGgvFky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecución guiada"
      ],
      "metadata": {
        "id": "6xAgdaccr8t7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del ejemplo pasado, se va a adentrar más a fondo que hace cada parte de Gym."
      ],
      "metadata": {
        "id": "bityVkV7zNXv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1fvnpzjp-rd"
      },
      "source": [
        "### Configuración del ambiente de Python\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6950MLK9p-rd"
      },
      "outputs": [],
      "source": [
        "# Author: Till Zemann\n",
        "# Modifier: Luis Solano\n",
        "# License: MIT License\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear el ambiente de Blackjack, siguiedo la implementación del libro de Sutton & Barto."
      ],
      "metadata": {
        "id": "FyQvlgKXvt_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Blackjack-v1\", sab=True)"
      ],
      "metadata": {
        "id": "P4biXl08vtNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observando el ambiente"
      ],
      "metadata": {
        "id": "LlHgYrpOxer_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCUDNoWTp-re"
      },
      "source": [
        "Primero, llamamos a ``env.reset()`` para iniciar un episodio. Esta función reinicia el entorno a una posición inicial y devuelve una ``observación`` inicial. Normalmente también establecemos ``done = False``. Esta variable será útil más adelante para verificar si un juego ha terminado (es decir, si el jugador gana o pierde).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmBL_nCMp-re"
      },
      "outputs": [],
      "source": [
        "# Reset the environment to get the first observation\n",
        "done = False\n",
        "observation, info = env.reset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNp7vjfUp-rf"
      },
      "source": [
        "Tengamos en cuenta que nuestra observación es una tupla que consiste en 3 valores:\n",
        "\n",
        "- La suma actual del jugador\n",
        "- El valor de la carta visible del crupier\n",
        "- Un valor booleano que indica si el jugador tiene un as utilizable (un as es utilizable si cuenta como 11 sin pasarse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI7uDM2-p-rf"
      },
      "source": [
        "### Ejecutando una acción\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Después de recibir nuestra primera observación, solo vamos a usar la función ``env.step(action)`` para interactuar con el entorno. Esta función toma una acción como entrada y la ejecuta en el entorno. Debido a que esa acción cambia el estado del entorno, nos devuelve cuatro variables útiles. Estas son:\n",
        "\n",
        "- ``next_state``: Esta es la observación que el agente recibirá después de realizar la acción.\n",
        "- ``reward``: Esta es la recompensa que el agente recibirá después de realizar la acción.\n",
        "- ``terminated``: Esta es una variable booleana que indica si el entorno ha terminado o no.\n",
        "- ``truncated``: Esta es una variable booleana que también indica si el episodio terminó por truncamiento anticipado, es decir, si se alcanzó un límite de tiempo.\n",
        "- ``info``: Este es un diccionario que puede contener información adicional sobre el entorno.\n",
        "\n",
        "Hay que tomar en cuenta que no es una buena idea llamar a ``env.render()`` en el ciclo de entrenamiento porque renderizar ralentiza mucho el entrenamiento. En su lugar, se puede construir un ciclo adicional para evaluar y mostrar el agente después del entrenamiento, como se hizo previamente.\n"
      ],
      "metadata": {
        "id": "FpEZls-wxmaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeOjygzcp-rg"
      },
      "outputs": [],
      "source": [
        "# sample a random action from all valid actions\n",
        "action = env.action_space.sample()\n",
        "# action=1\n",
        "\n",
        "# execute the action in our environment and receive infos from the environment\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "# observation=(24, 10, False)\n",
        "# reward=-1.0\n",
        "# terminated=True\n",
        "# truncated=False\n",
        "# info={}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bavRZ0wp-rg"
      },
      "source": [
        "Una vez que ``terminated = True`` o ``truncated=True``, debemos detener el\n",
        "episodio actual y comenzar uno nuevo con ``env.reset()``. Si sigues\n",
        "ejecutando acciones sin reiniciar el entorno, este seguirá respondiendo,\n",
        "pero la salida no será útil para el entrenamiento (incluso podría ser\n",
        "perjudicial si el agente aprende con datos inválidos).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construyendo una clase agente"
      ],
      "metadata": {
        "id": "LCk9YaoMzT21"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZayBnezp-rh"
      },
      "source": [
        "\n",
        "\n",
        "A cambio del ejemplo pasado, aquí se construye una clase agente, en caso de no conocer programación orientada a objetos pueden preguntarme (si están presencialmente en el laboratorio) y puedo ayudarlos.\n",
        "\n",
        "¡Construyamos un ``agente de Q-learning`` para resolver *Blackjack-v1*! Necesitaremos\n",
        "algunas funciones para seleccionar una acción y actualizar los valores de acción del agente.\n",
        "Para asegurarnos de que el agente explore el entorno, una posible\n",
        "solución es la estrategia ``epsilon-greedy``, donde seleccionamos una acción aleatoria con el porcentaje ``epsilon`` y la acción greedy (actualmente valorada como la mejor) con ``1 - epsilon``.\n",
        "\n",
        "Este es el mismo algoritmo que utilizamos la vez pasada, pueden profundizar más por su cuenta en este [enlace](https://www.google.com/search?client=firefox-b-d&q=epsilon+greedy+selection).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXJDfRZAp-rh"
      },
      "outputs": [],
      "source": [
        "class BlackjackAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        learning_rate: float,\n",
        "        initial_epsilon: float,\n",
        "        epsilon_decay: float,\n",
        "        final_epsilon: float,\n",
        "        discount_factor: float = 0.95,\n",
        "    ):\n",
        "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
        "        of state-action values (q_values), a learning rate and an epsilon.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: The learning rate\n",
        "            initial_epsilon: The initial epsilon value\n",
        "            epsilon_decay: The decay for epsilon\n",
        "            final_epsilon: The final epsilon value\n",
        "            discount_factor: The discount factor for computing the Q-value\n",
        "        \"\"\"\n",
        "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.final_epsilon = final_epsilon\n",
        "\n",
        "        self.training_error = []\n",
        "\n",
        "    def get_action(self, env, obs: tuple[int, int, bool]) -> int:\n",
        "        \"\"\"\n",
        "        Returns the best action with probability (1 - epsilon)\n",
        "        otherwise a random action with probability epsilon to ensure exploration.\n",
        "        \"\"\"\n",
        "        # with probability epsilon return a random action to explore the environment\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return env.action_space.sample()\n",
        "\n",
        "        # with probability (1 - epsilon) act greedily (exploit)\n",
        "        else:\n",
        "            return int(np.argmax(self.q_values[obs]))\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        obs: tuple[int, int, bool],\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "        next_obs: tuple[int, int, bool],\n",
        "    ):\n",
        "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
        "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
        "        temporal_difference = (\n",
        "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
        "        )\n",
        "\n",
        "        self.q_values[obs][action] = (\n",
        "            self.q_values[obs][action] + self.lr * temporal_difference\n",
        "        )\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhsmYzx-p-rh"
      },
      "source": [
        "Para entrenar al agente, dejaremos que el agente juegue un episodio (un juego completo se llama episodio) a la vez y luego actualizaremos sus valores Q después de cada paso (una acción única en un juego se llama paso).\n",
        "\n",
        "El agente tendrá que experimentar muchos episodios para explorar el\n",
        "entorno de manera suficiente.\n",
        "\n",
        "Ahora deberíamos estar listos para construir el ciclo de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhZSMSExp-ri"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "n_episodes = 100_000\n",
        "start_epsilon = 1.0\n",
        "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
        "final_epsilon = 0.1\n",
        "\n",
        "agent = BlackjackAgent(\n",
        "    env=env,\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=start_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=final_epsilon,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gn44vgTp-ri"
      },
      "source": [
        "¡Genial, entrenemos!\n",
        "\n",
        "Información: Los hiperparámetros actuales están configurados para entrenar rápidamente un agente decente.\n",
        "\n",
        "Si se desea converger a la política óptima, se puede aumentar\n",
        "el número de episodios (n_episodes) por 10 veces y reducir la tasa de aprendizaje (por ejemplo, a 0.001).\n",
        "\n",
        "Experimenten :)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxVX9P9lp-ri",
        "outputId": "c17a9414-9027-452a-c6a2-e18e4b435c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [00:18<00:00, 5345.54it/s]\n"
          ]
        }
      ],
      "source": [
        "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # play one episode\n",
        "    while not done:\n",
        "        action = agent.get_action(env, obs)\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # update the agent\n",
        "        agent.update(obs, action, reward, terminated, next_obs)\n",
        "\n",
        "        # update if the environment is done and the current obs\n",
        "        done = terminated or truncated\n",
        "        obs = next_obs\n",
        "\n",
        "    agent.decay_epsilon()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Juego de resultado"
      ],
      "metadata": {
        "id": "hx_ZeZYG07YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, el agente no tiene soporte para video entonces una interfaz de texto mostrará los resultados."
      ],
      "metadata": {
        "id": "h2q3u4CF688y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_blackjack(agent, env):\n",
        "    \"\"\"\n",
        "    Plays a single game of Blackjack with the given agent.\n",
        "    The agent decides actions until the game ends, and the outcome is printed.\n",
        "    \"\"\"\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    print(\"\\n--- Starting a new Blackjack game ---\")\n",
        "    print(f\"Initial state (Player Total, Dealer Card, Usable Ace): {state}\\n\")\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(env, state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        action_str = \"Hit\" if action == 1 else \"Stick\"\n",
        "        print(f\"Agent chose {action_str} -> New state: {next_state}\")\n",
        "\n",
        "        agent.update(state, action, reward, done, next_state)\n",
        "        state = next_state\n",
        "\n",
        "    print(\"\\n--- Game Over ---\")\n",
        "    if reward > 0:\n",
        "        print(\"Agent **WON** the game!\")\n",
        "    elif reward < 0:\n",
        "        print(\"Agent **LOST** the game.\")\n",
        "    else:\n",
        "        print(\"It's a **TIE**.\")\n",
        "\n",
        "    agent.decay_epsilon()"
      ],
      "metadata": {
        "id": "jbawa8nD6wQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_blackjack(agent, env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43mq0CmR6Urz",
        "outputId": "28f0db4c-8160-43b4-f5a3-90f8d0988f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting a new Blackjack game ---\n",
            "Initial state (Player Total, Dealer Card, Usable Ace): (15, 1, 0)\n",
            "\n",
            "Agent chose Stick -> New state: (15, 1, 0)\n",
            "\n",
            "--- Game Over ---\n",
            "Agent **LOST** the game.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}